{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22e720dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For some reasons, file descriptors (FDs) do not get released.\n",
    "# This is a work around which increases the allowed limit.\n",
    "import resource\n",
    "rlimit = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
    "resource.setrlimit(resource.RLIMIT_NOFILE, (2048, rlimit[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11cff375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ahmedmohammed/opt/miniconda3/envs/pytorch_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6a2f9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "train_B = 16\n",
    "val_B = 1\n",
    "\n",
    "SCALE = True\n",
    "data_folder = 'BH average 100-sps (0.75-38 Hz)'\n",
    "files_names = os.listdir(data_folder)\n",
    "\n",
    "g1 = ['PAT31', 'MC', 'FEsg', 'DHut', 'NKra', 'PAT2']\n",
    "g2 = ['RC', 'MBra', 'ESow', 'PAT1', 'EG', 'FigSa', 'PAT28']\n",
    "g3 = ['PAT47', 'RA', 'PAT51', 'MPi', 'GNA', 'LM', 'DG']\n",
    "g4 = ['PAT33', 'PB', 'PAT22', 'MAXJ', 'PAT19', 'LP', 'HeMod', 'PAT50', 'PJuly']\n",
    "g5 = ['PAT48', 'KS', 'PAT3', 'PAT8', 'LRio', 'HB', 'ALo', 'JAlv', 'PAT25', 'AR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff1ade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_patients = g2 + g3 + g4 + g5\n",
    "val_patients = g1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a74bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, segments, labels, flip_prob, montage):\n",
    "        self.segments = segments\n",
    "        self.labels = labels\n",
    "        self.flip_prob = flip_prob\n",
    "        \n",
    "        if flip_prob > 0:\n",
    "            self.idx_swaps = self.montage_swap_func(montage)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def montage_swap_func(montage):\n",
    "        if montage == 'BH':\n",
    "            electrodes = ['Fp1','F7','T3','T5','O1','F3','C3','P3','Fz','Cz','Pz','Fp2','F8','T4','T6','O2','F4','C4','P4']\n",
    "        elif montage == 'MCH':\n",
    "            electrodes = ['C3','C4','O1','O2','Cz','F3','F4','F7','F8','Fz','Fp1','Fp2','P3','P4','Pz','T3','T4','T5','T6']\n",
    "        else:\n",
    "            raise NameError('Unavailable Montage')\n",
    "        \n",
    "        dic = {electrode: idx for idx, electrode in enumerate(electrodes)}\n",
    "        \n",
    "        label_swaps = [('Fp1', 'Fp2'), ('F7', 'F8'), ('F3', 'F4'), ('C3', 'C4'), ('P3', 'P4'), ('T3', 'T4'), ('T5', 'T6'), ('O1', 'O2')]\n",
    "        return [(dic[e_left], dic[e_right]) for e_left, e_right in label_swaps]\n",
    "    \n",
    "    \n",
    "    def flip_lr(self, segment):\n",
    "        # flip based on self.montage\n",
    "        for e_left, e_right in self.idx_swaps:\n",
    "            cloned_channel = torch.clone(segment[e_left])\n",
    "            segment[e_left] = segment[e_right]\n",
    "            segment[e_right] = cloned_channel\n",
    "        \n",
    "        return segment\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if random.random() < self.flip_prob:\n",
    "            return self.flip_lr(torch.clone(self.segments[index])), self.labels[index]\n",
    "        else:\n",
    "            return self.segments[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e5c9bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_patient_data(patient_name):\n",
    "    \n",
    "    patient_segments = []\n",
    "    patient_labels = []\n",
    "    #patient_filesnames = []\n",
    "    \n",
    "    regex = re.compile(f'{patient_name}\\D[0-9.a-z_A-Z]*')\n",
    "    \n",
    "    for file_name in files_names:\n",
    "        if regex.match(file_name):\n",
    "            segment = torch.tensor( loadmat(os.path.join(data_folder, file_name))['segment'] ).float()\n",
    "            if 'sp' in file_name:\n",
    "                label = torch.tensor(1.0).float()\n",
    "            elif 'ns' in file_name:\n",
    "                label = torch.tensor(0.0).float()\n",
    "            \n",
    "            patient_segments.append(segment)\n",
    "            patient_labels.append(label)\n",
    "            #patient_filesnames.append(file_name)\n",
    "    \n",
    "    #return patient_segments, patient_labels, patient_filesnames\n",
    "    return patient_segments, patient_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f68c8e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(patients_names):\n",
    "    group_segments = []\n",
    "    group_labels = []\n",
    "    #group_filesnames = []\n",
    "    \n",
    "    #with mp.Pool(processes=mp.cpu_count() - 1) as p:\n",
    "    #    #for patient_segments, patient_labels, patient_filesnames in p.imap_unordered(prep_patient_data, patients_names):\n",
    "    #    for patient_segments, patient_labels in p.imap_unordered(prep_patient_data, patients_names):\n",
    "    #        group_segments += patient_segments\n",
    "    #        group_labels += patient_labels\n",
    "    #        #group_filesnames += patient_filesnames\n",
    "    \n",
    "    for patient_name in patients_names:\n",
    "        patient_segments, patient_labels = prep_patient_data(patient_name)\n",
    "        group_segments += patient_segments\n",
    "        group_labels += patient_labels\n",
    "    \n",
    "    #return group_segments, group_labels, group_filesnames\n",
    "    return group_segments, group_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a3441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_segments, train_labels, train_filesnames = prepare_dataset(train_patients)\n",
    "#val_segments, val_labels, val_filesnames = prepare_dataset(val_patients)\n",
    "train_segments, train_labels = prepare_dataset(train_patients)\n",
    "val_segments, val_labels = prepare_dataset(val_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "def059e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_IQR_func(group_segments):\n",
    "    numbers = []\n",
    "    for segment in group_segments:\n",
    "        for row in range(segment.shape[0]):\n",
    "            for col in range(segment.shape[1]):\n",
    "                numbers.append(segment[row, col].item())\n",
    "    \n",
    "    quantiles = torch.tensor(numbers).quantile( torch.tensor([0.25, 0.5, 0.75]) )\n",
    "    median = quantiles[1].item()\n",
    "    IQR = quantiles[2].item() - quantiles[0].item()\n",
    "    \n",
    "    return median, IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec5cb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler_func(group_segments, median, IQR):\n",
    "    # Scales the group_segments list inplace\n",
    "    for idx in range(len(group_segments)):\n",
    "        group_segments[idx] = ((group_segments[idx] - median) / IQR)\n",
    "    \n",
    "    return group_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a2331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCALE:\n",
    "    median, IQR = median_IQR_func(train_segments)\n",
    "    train_segments = scaler_func(train_segments, median, IQR)\n",
    "    val_segments = scaler_func(val_segments, median, IQR)\n",
    "\n",
    "#train_dataset = CustomDataset(train_segments, train_labels, train_filesnames)\n",
    "#val_dataset = CustomDataset(val_segments, val_labels, val_filesnames)\n",
    "train_dataset = CustomDataset(train_segments, train_labels, 0.5, 'BH')\n",
    "val_dataset = CustomDataset(val_segments, val_labels, 0., 'BH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a3f727",
   "metadata": {},
   "outputs": [],
   "source": [
    "del files_names, train_segments, train_labels, val_segments, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_validation_loss = math.inf\n",
    "best_epoch = 0\n",
    "best_model_state = None\n",
    "AP_at_min_val_loss = 0.\n",
    "EARLY_STOPPING = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d47a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With no padding: L_out = ( L_in - (L_kernel - 1) ) / stride\n",
    "class CustomLightningModule(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # input: (B, 19, 300)\n",
    "        self.seq = torch.nn.Sequential(\n",
    "            # input: (B, 19, 300) ===> ( B, 32, (300 - (3 - 1)) / 1 ) = (B, 32, 298)\n",
    "            torch.nn.Conv1d(in_channels=19, out_channels=32, kernel_size=3, padding='valid'),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            # torch.nn.Dropout(p=0.3),\n",
    "\n",
    "            # (B, 32, 298) ===> ( B, 32, (298 - (3 - 1)) / 1 ) = (B, 32, 296)\n",
    "            torch.nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, padding='valid'),\n",
    "            torch.nn.LeakyReLU(),\n",
    "\n",
    "            # (B, 32, 296) ===> (B, 32, 148)\n",
    "            #torch.nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            torch.nn.Conv1d(in_channels=32, out_channels=32, kernel_size=2, stride=2),\n",
    "            torch.nn.LeakyReLU(),\n",
    "\n",
    "            # (B, 32, 148) ===> ( B, 16, (148 - (3 - 1)) / 1 ) = (B, 16, 146)\n",
    "            torch.nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, padding='valid'),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            # torch.nn.Dropout(p=0.3),\n",
    "\n",
    "            # (B, 16, 146) ===> ( B, 16, (146 - (3 - 1)) / 1 ) = (B, 16, 144)\n",
    "            torch.nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding='valid'),\n",
    "            torch.nn.LeakyReLU(),\n",
    "\n",
    "            # (B, 16, 144) ===> (B, 16, 72)\n",
    "            #torch.nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            torch.nn.Conv1d(in_channels=16, out_channels=16, kernel_size=2, stride=2),\n",
    "            torch.nn.LeakyReLU(),\n",
    "\n",
    "            # (B, 16, 72) ===> ( B, 8, (72 - (3 - 1)) / 1 ) = (B, 8, 70)\n",
    "            torch.nn.Conv1d(in_channels=16, out_channels=8, kernel_size=3, padding='valid'),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            # torch.nn.Dropout(p=0.3),\n",
    "\n",
    "            # (B, 8, 70) ===> ( B, 8, (70 - (3 - 1)) / 1 ) = (B, 8, 68)\n",
    "            torch.nn.Conv1d(in_channels=8, out_channels=8, kernel_size=3, padding='valid'),\n",
    "            torch.nn.LeakyReLU(),\n",
    "\n",
    "            # (B, 8, 68) ===> (B, 8, 34)\n",
    "            #torch.nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            torch.nn.Conv1d(in_channels=8, out_channels=8, kernel_size=2, stride=2),\n",
    "            torch.nn.LeakyReLU(),\n",
    "\n",
    "            # (B, 8, 34) ===> ( B, 4, (34 - (3 - 1)) / 1 ) = (B, 4, 32)\n",
    "            torch.nn.Conv1d(in_channels=8, out_channels=4, kernel_size=3, padding='valid'),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            # torch.nn.Dropout(p=0.3),\n",
    "\n",
    "            # (B, 4, 32) ===> ( B, 4, (32 - (3 - 1)) / 1 ) = (B, 4, 30)\n",
    "            torch.nn.Conv1d(in_channels=4, out_channels=4, kernel_size=3, padding='valid'),\n",
    "            torch.nn.LeakyReLU(),\n",
    "\n",
    "            # (B, 4, 30) ===> (B, 4, 15)\n",
    "            #torch.nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            torch.nn.Conv1d(in_channels=4, out_channels=4, kernel_size=2, stride=2),\n",
    "            torch.nn.LeakyReLU(),\n",
    "\n",
    "            # (B, 4, 15) ===> ( B, 2, (15 - (3 - 1)) / 1 ) = (B, 2, 13)\n",
    "            torch.nn.Conv1d(in_channels=4, out_channels=2, kernel_size=3, padding='valid'),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            # torch.nn.Dropout(p=0.3),\n",
    "\n",
    "            # (B, 2, 13) ===> ( B, 2, (13 - (3 - 1)) / 1 ) = (B, 2, 11)\n",
    "            torch.nn.Conv1d(in_channels=2, out_channels=2, kernel_size=3, padding='valid'),\n",
    "            torch.nn.LeakyReLU(),\n",
    "\n",
    "            # CANNOT DO MAXPOOL\n",
    "\n",
    "            # (B, 2, 11) ===> ( B, 1, (11 - (3 - 1)) / 1 ) = (B, 1, 9)\n",
    "            torch.nn.Conv1d(in_channels=2, out_channels=1, kernel_size=3, padding='valid'),\n",
    "            torch.nn.LeakyReLU(),\n",
    "\n",
    "            # (B, 1, 9) ===> ( B, 1, (9 - (3 - 1)) / 1 ) = (B, 1, 7)\n",
    "            torch.nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, padding='valid'),\n",
    "            torch.nn.LeakyReLU(),\n",
    "\n",
    "            # (B, 1, 7) ===> ( B, 1, (7 - (3 - 1)) / 1 ) = (B, 1, 5)\n",
    "            torch.nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, padding='valid'),\n",
    "            torch.nn.LeakyReLU(),\n",
    "\n",
    "            # (B, 1, 5) ===> ( B, 1, (5 - (3 - 1)) / 1 ) = (B, 1, 3)\n",
    "            torch.nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, padding='valid'),\n",
    "            torch.nn.LeakyReLU(),\n",
    "\n",
    "            # (B, 1, 3) ===> ( B, 1, (3 - (3 - 1)) / 1 ) = (B, 1, 1)\n",
    "            torch.nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, padding='valid')\n",
    "        )\n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, 19, 300)\n",
    "        B = x.shape[0]\n",
    "        return self.seq(x).reshape((B,))\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), weight_decay=0)\n",
    "        return [optimizer], [torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.99, min_lr=0.00001)]\n",
    "    \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(train_dataset, batch_size=train_B, shuffle=True,\n",
    "                                           num_workers=0, drop_last=True)\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        #torch._assert(x.shape == (train_B, 19, 300) , f'x.shape = {x.shape} BUT it should be ({train_B}, 19, 300)')\n",
    "        \n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            logits_before_optimizer_step = self(x)\n",
    "            loss_before_optimizer_step = self.criterion(logits_before_optimizer_step, y)\n",
    "        \n",
    "        self.train()\n",
    "        \n",
    "        # Note that this is NOT a pytorch optimizer. I think it's a wrapper\n",
    "        opt = self.optimizers()\n",
    "\n",
    "        def closure():\n",
    "            logits = self(x)\n",
    "\n",
    "            #torch._assert(logits.shape == y.shape , f'@ Training ====> logits.shape = {logits.shape} != y.shape = {y.shape}')\n",
    "\n",
    "            loss = self.criterion(logits, y)\n",
    "            opt.zero_grad()\n",
    "            self.manual_backward(loss)\n",
    "            return loss\n",
    "\n",
    "        opt.step(closure=closure)\n",
    "\n",
    "        # I think since I am doing manual optimization, there is no need to return anything\n",
    "        return {'loss_before_optimizer_step': loss_before_optimizer_step.detach()}\n",
    "    \n",
    "    \n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        if self.global_rank == 0:\n",
    "            train_loss = torch.stack([dic['loss_before_optimizer_step'] for dic in training_step_outputs]).mean()\n",
    "            print(f'Epoch: {self.current_epoch}')\n",
    "            print(f'Training loss BEFORE optimizer step: {round(train_loss.item(), 5)}')\n",
    "            print('--------------------------------------')\n",
    "    \n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(val_dataset, batch_size=val_B, shuffle=False,\n",
    "                                           num_workers=0, drop_last=False)\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        #torch._assert(x.shape == (val_B, 19, 300) , f'x.shape = {x.shape} BUT it should be ({val_B}, 19, 300)')\n",
    "\n",
    "        logits = self(x)\n",
    "\n",
    "        #torch._assert(logits.shape == y.shape , f'@ Val ====> logits.shape = {logits.shape} != y.shape = {y.shape}')\n",
    "\n",
    "        loss = self.criterion(logits, y)\n",
    "\n",
    "        return {'loss': loss, 'y': y, 'logits': logits}\n",
    "    \n",
    "    \n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        global min_validation_loss, best_epoch, best_model_state, AP_at_min_val_loss\n",
    "        \n",
    "        val_loss = torch.stack([dic['loss'] for dic in validation_step_outputs]).mean()\n",
    "        \n",
    "        \n",
    "        if self.global_rank == 0:\n",
    "            \n",
    "            val_loss = torch.stack([dic['loss'] for dic in validation_step_outputs]).mean()\n",
    "            val_loss = val_loss.item()\n",
    "            print(f'val_loss: {val_loss}')\n",
    "            \n",
    "            y = torch.cat([dic['y'] for dic in validation_step_outputs]).cpu()\n",
    "            logits = torch.cat([dic['logits'] for dic in validation_step_outputs]).cpu()\n",
    "            pred = torch.sigmoid(logits)\n",
    "            \n",
    "            y = np.array(y)\n",
    "            pred = np.array(pred)\n",
    "            \n",
    "            try:\n",
    "                validation_AP = average_precision_score(y, pred)\n",
    "                \n",
    "                if (val_loss < min_validation_loss) and (self.current_epoch > 1):\n",
    "                    min_validation_loss = val_loss\n",
    "                    best_epoch = self.current_epoch\n",
    "                    best_model_state = deepcopy(self.state_dict())\n",
    "                    AP_at_min_val_loss = validation_AP\n",
    "                \n",
    "                \n",
    "                print(f'validation_AP = {round(validation_AP, 5)} ... AP_at_min_val_loss = {round(AP_at_min_val_loss, 5)} @ epoch {best_epoch}')\n",
    "                \n",
    "            except ValueError:\n",
    "                print(f'ValueError @ epoch: {self.current_epoch} ====> y = {y}')\n",
    "            \n",
    "            print('=======================================================================================')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b9f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lightning_module = CustomLightningModule()\n",
    "trainer = pl.Trainer(gpus=0, enable_checkpointing=False, enable_progress_bar=False, logger=False, max_epochs=500)\n",
    "trainer.fit(my_lightning_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d285f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = f'models/g1_vanilla_vgg_10k_AP_{round(100 * AP_at_min_val_loss, 2)}.pt'\n",
    "torch.save(best_model_state, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2d007b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef77061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9540bee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b9fa001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "model = CustomLightningModule()\n",
    "\n",
    "#IEDs_names = [item for item in os.listdir(data_folder) if 'sp' in item]\n",
    "NIEDs_names = [item for item in os.listdir(data_folder) if 'ns' in item]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d81f14f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = [item for item in os.listdir('models/') if 'g1_vanilla_vgg' in item]\n",
    "g1_model_name = max(lis)\n",
    "train_segments, _ = prepare_dataset(g2 + g3 + g4 + g5)\n",
    "g1_median, g1_IQR = median_IQR_func(train_segments)\n",
    "\n",
    "lis = [item for item in os.listdir('models/') if 'g2_vanilla_vgg' in item]\n",
    "g2_model_name = max(lis)\n",
    "train_segments, _ = prepare_dataset(g1 + g3 + g4 + g5)\n",
    "g2_median, g2_IQR = median_IQR_func(train_segments)\n",
    "\n",
    "\n",
    "lis = [item for item in os.listdir('models/') if 'g3_vanilla_vgg' in item]\n",
    "g3_model_name = max(lis)\n",
    "train_segments, _ = prepare_dataset(g1 + g2 + g4 + g5)\n",
    "g3_median, g3_IQR = median_IQR_func(train_segments)\n",
    "\n",
    "\n",
    "lis = [item for item in os.listdir('models/') if 'g4_vanilla_vgg' in item]\n",
    "g4_model_name = max(lis)\n",
    "train_segments, _ = prepare_dataset(g1 + g2 + g3 + g5)\n",
    "g4_median, g4_IQR = median_IQR_func(train_segments)\n",
    "\n",
    "\n",
    "lis = [item for item in os.listdir('models/') if 'g5_vanilla_vgg' in item]\n",
    "g5_model_name = max(lis)\n",
    "train_segments, _ = prepare_dataset(g1 + g2 + g3 + g4)\n",
    "g5_median, g5_IQR = median_IQR_func(train_segments)\n",
    "\n",
    "\n",
    "#s = pd.Series(index=IEDs_names, dtype='float64')\n",
    "s = pd.Series(index=NIEDs_names, dtype='float64')\n",
    "\n",
    "\n",
    "#for IED_name in IEDs_names:\n",
    "for NIED_name in NIEDs_names:\n",
    "    #segment = loadmat(os.path.join(data_folder, IED_name))['segment']\n",
    "    segment = loadmat(os.path.join(data_folder, NIED_name))['segment']\n",
    "    segment = torch.tensor(segment).float()\n",
    "    \n",
    "    #patient_name = IED_name[: IED_name.find('_')]\n",
    "    patient_name = NIED_name[: NIED_name.find('_')]\n",
    "    \n",
    "    if patient_name in g1:\n",
    "        segment = (segment - g1_median) / g1_IQR\n",
    "        model.load_state_dict(torch.load(f'models/{g1_model_name}'))\n",
    "    elif patient_name in g2:\n",
    "        segment = (segment - g2_median) / g2_IQR\n",
    "        model.load_state_dict(torch.load(f'models/{g2_model_name}'))\n",
    "    elif patient_name in g3:\n",
    "        segment = (segment - g3_median) / g3_IQR\n",
    "        model.load_state_dict(torch.load(f'models/{g3_model_name}'))\n",
    "    elif patient_name in g4:\n",
    "        segment = (segment - g4_median) / g4_IQR\n",
    "        model.load_state_dict(torch.load(f'models/{g4_model_name}'))\n",
    "    elif patient_name in g5:\n",
    "        segment = (segment - g5_median) / g5_IQR\n",
    "        model.load_state_dict(torch.load(f'models/{g5_model_name}'))\n",
    "    else:\n",
    "        raise NameError('Unknown Patient')\n",
    "    \n",
    "    \n",
    "    segment.unsqueeze_(0)\n",
    "    model.eval()\n",
    "    \n",
    "    #s.at[IED_name] = torch.sigmoid(model(segment)).item()\n",
    "    s.at[NIED_name] = torch.sigmoid(model(segment)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ff847bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RC_24_ns.mat      0.005738\n",
       "FEsg_38_ns.mat    0.000087\n",
       "HB_2_ns.mat       0.050953\n",
       "FEsg_26_ns.mat    0.103463\n",
       "FigSa_9_ns.mat    0.004070\n",
       "                    ...   \n",
       "LM_3_ns.mat       0.058508\n",
       "RC_6_ns.mat       0.004121\n",
       "FEsg_19_ns.mat    0.003594\n",
       "NKra_10_ns.mat    0.000157\n",
       "PAT48_ns_6.mat    0.029589\n",
       "Length: 593, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e8aece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with pd.ExcelWriter('IEDs_results.xlsx', mode='a') as writer:\n",
    "with pd.ExcelWriter('NIEDs_results.xlsx', mode='a') as writer:\n",
    "    s.to_excel(writer, sheet_name='vanilla_vgg', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9e102a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa9d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19989d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
